\section {Тестирование} \label{ch05-testing}

Как было изложено в разделе \ref{ch04-testing}, для тестирования нами было выбрано случайное подмножество из 100 синсетов PWN, для 97 из которых было найдено хотя бы по одну кандидату на связывание. В этом разделе мы опишем ход и результаты второго этапа тестирования, исследующего эффективность процедуры выравнивания в целом (т.е. комбинацию автоматического выравнивания и применения краудсорсинга).

В процессе тестирования было проведено два раунда. В первом было обработано 159 заданий, во втором --- 57 заданий. В обоих раундах было использовано перекрытие (количество раз, которое будет выполнено каждое задание разными рабочими) 5.

Задания для первого раунда были сформированы на основании графа связей. 16 из 57 заданий второго раунда также были получены только на основании связей, полученных автоматическим выравниванием (синсеты этих заданий не были включены в первый раунд из организационных соображений), другие 39 заданий второго раунда были сгенерированны на основании результатов первого раунда.

Как было отмечено в \ref{ch04-testing}, тестирование метода проводилось путем сравнения с эталонной разметкой. С помощью краудсорсинга мы получили для каждого исходного синсета из PWN от 0 до 2 синсетов-победителей, сравнили полученные данные c эталоном. В \inlref{таблице}{tbl:testing-result} представлено сравнение результатов.

\begin{table}[tb]
\caption{Сравнение результатов краудсорсинга и экспертной разметки.\label{tbl:testing-result}}
\begin{tabular}{|l r|}
\toprule
 Результат & Кол-во \\
\midrule
 Нет перевода & 3 \\
 Неверная классификация, "ни один не подходит" & 1 \\
 Неверная классификация, нерелевантный синсет & 12 \\
 Согласующаяся классификация, "ни один не подходит" & 5 \\
 Согласующаяся классификация, релевантный синсет & 79 \\
\bottomrule
\end{tabular}
\end{table}

Причем, из 79 классифицированных согласующимся с мнением эксперта образом синсетов, 56 получившихся классификаций были отмечены экспертом, как полностью соответствующие.
Остальные 23 установленные связи требуют дополнительного уточнения синсетов из YARN, чтобы соответствующие синсеты также полностью соответствовали понятиям, описанным исходными синсетами PWN.

Однако установленные связи с синсетами, не полностью соответствующими понятиям (но достаточно близкими) --- еще не столь большая проблема. То же касается и синсетов, для которых неверно помечено, что ни один кандидат не подходит. Гораздо более значительную проблему представляют синсеты, для которых были выбраны некорректные кандидаты. Мы подробно проанализировали причины неверной классификации:

* смешение понятий в синсете YARN: 1
* выбран нерелевантный синсет при отсутствии релевантного в выборке
    + выбран синсет, соответствующий английскому понятию, имеющим общие слова с исходным: 2
* выбран нерелевантный синсет при наличии релевантного в выборке
    + выбран синсет, соответствующий английскому понятию, имеющим общие слова с исходным: 5
    + выбран слишком общий гипероним: 1
    + выбран слишком частный гипоним: 3
* не выбрано ни одного при наличии релевантного синсета в выборке: 1

Что замечено, часто синсеты успешно проходили первый шаг (т.е. в множестве кандидатов по результатам голосования на первом шаге оставались в том числе релевантные синсеты), однако на втором участники голосования выбирали ошибочный синсет. На самом деле 12 синсетов (или 12% тестовой выборки) --- достаточно неплохой результат. Однако для построения тезауруса (а задача выравнивания формулируется именно в как подзадача задачи построения тезауруса YARN) точность 88% не является
удовлетворительной. В следующем разделе мы рассмотрим подходы, применяя которые мы надеямся улучшить полученный результат (однако применение которых выходит за рамки настоящей работы).
